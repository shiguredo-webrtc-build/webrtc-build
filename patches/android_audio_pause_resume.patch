diff --git a/api/audio/audio_device.h b/api/audio/audio_device.h
index fe3ef15a9a..1075bc8411 100644
--- a/api/audio/audio_device.h
+++ b/api/audio/audio_device.h
@@ -17,6 +17,10 @@
 #include "api/audio/audio_device_defines.h"
 #include "api/ref_count.h"
 
+#if defined(WEBRTC_ANDROID)
+#include <jni.h>
+#endif
+
 namespace webrtc {
 
 class AudioDeviceModuleForTest;
@@ -102,6 +106,14 @@ class AudioDeviceModule : public RefCountInterface {
   virtual int32_t StopRecording() = 0;
   virtual bool Recording() const = 0;
 
+#if defined(WEBRTC_ANDROID)
+  // Optional support for pausing and resuming recording on Android.
+  // Default implementations return false, indicating not supported.
+  virtual bool PauseRecording(JNIEnv* env) { return false; }
+  virtual bool ResumeRecording(JNIEnv* env) { return false; }
+  virtual bool SupportsPauseRecording(JNIEnv* env) const { return false; }
+#endif
+
   // Audio mixer initialization
   virtual int32_t InitSpeaker() = 0;
   virtual bool SpeakerIsInitialized() const = 0;
diff --git a/sdk/android/api/org/webrtc/audio/JavaAudioDeviceModule.java b/sdk/android/api/org/webrtc/audio/JavaAudioDeviceModule.java
index bbf1c3a753..4dc7da9881 100644
--- a/sdk/android/api/org/webrtc/audio/JavaAudioDeviceModule.java
+++ b/sdk/android/api/org/webrtc/audio/JavaAudioDeviceModule.java
@@ -419,6 +419,50 @@ public class JavaAudioDeviceModule implements AudioDeviceModule {
     audioInput.setMicrophoneMute(mute);
   }
 
+  /**
+   * Temporarily pause microphone capture. Returns true if the request succeeded or recording was
+   * already paused.
+   */
+  public boolean pauseRecording() {
+    Logging.d(TAG, "pauseRecording");
+    synchronized (nativeLock) {
+      if (nativeAudioDeviceModule != 0) {
+        if (!nativeSupportsPauseRecording(nativeAudioDeviceModule)) {
+          Logging.w(TAG, "pauseRecording not supported by native ADM");
+          return false;
+        }
+        boolean result = nativePauseRecording(nativeAudioDeviceModule);
+        Logging.d(TAG, "pauseRecording native result=" + result);
+        return result;
+      }
+    }
+    boolean result = audioInput.pauseRecording();
+    Logging.d(TAG, "pauseRecording java result=" + result);
+    return result;
+  }
+
+  /**
+   * Resume microphone capture after a pause. Returns true if the request succeeded or capture was
+   * already active.
+   */
+  public boolean resumeRecording() {
+    Logging.d(TAG, "resumeRecording");
+    synchronized (nativeLock) {
+      if (nativeAudioDeviceModule != 0) {
+        if (!nativeSupportsPauseRecording(nativeAudioDeviceModule)) {
+          Logging.w(TAG, "resumeRecording not supported by native ADM");
+          return false;
+        }
+        boolean result = nativeResumeRecording(nativeAudioDeviceModule);
+        Logging.d(TAG, "resumeRecording native result=" + result);
+        return result;
+      }
+    }
+    boolean result = audioInput.resumeRecording();
+    Logging.d(TAG, "resumeRecording java result=" + result);
+    return result;
+  }
+
   @Override
   public boolean setNoiseSuppressorEnabled(boolean enabled) {
     Logging.d(TAG, "setNoiseSuppressorEnabled: " + enabled);
@@ -441,4 +485,7 @@ public class JavaAudioDeviceModule implements AudioDeviceModule {
       AudioManager audioManager, WebRtcAudioRecord audioInput, WebRtcAudioTrack audioOutput,
       long webrtcEnvRef, int inputSampleRate, int outputSampleRate, boolean useStereoInput,
       boolean useStereoOutput);
+  private static native boolean nativeSupportsPauseRecording(long nativeAudioDeviceModule);
+  private static native boolean nativePauseRecording(long nativeAudioDeviceModule);
+  private static native boolean nativeResumeRecording(long nativeAudioDeviceModule);
 }
diff --git a/sdk/android/src/java/org/webrtc/audio/WebRtcAudioRecord.java b/sdk/android/src/java/org/webrtc/audio/WebRtcAudioRecord.java
index ac62308aa3..81b5939aab 100644
--- a/sdk/android/src/java/org/webrtc/audio/WebRtcAudioRecord.java
+++ b/sdk/android/src/java/org/webrtc/audio/WebRtcAudioRecord.java
@@ -79,6 +79,13 @@ class WebRtcAudioRecord {
   // directly after start.
   private static final int CHECK_REC_STATUS_DELAY_MS = 100;
 
+  private enum RecordingState {
+    IDLE,
+    RECORDING,
+    PAUSED,
+    STOPPED
+  }
+
   private final Context context;
   private final AudioManager audioManager;
   private final int audioSource;
@@ -108,6 +115,8 @@ class WebRtcAudioRecord {
   private final boolean isAcousticEchoCancelerSupported;
   private final boolean isNoiseSuppressorSupported;
 
+  private RecordingState recordingState = RecordingState.IDLE;
+
   /**
    * Audio thread which keeps calling ByteBuffer.read() waiting for audio
    * to be recorded. Feeds recorded data to the native counterpart as a
@@ -374,6 +383,14 @@ class WebRtcAudioRecord {
   @CalledByNative
   private boolean startRecording() {
     Logging.d(TAG, "startRecording");
+    if (recordingState == RecordingState.RECORDING) {
+      Logging.w(TAG, "startRecording invoked while already recording");
+      return true;
+    }
+    if (recordingState == RecordingState.PAUSED) {
+      Logging.e(TAG, "startRecording not allowed while paused; call resumeRecording instead");
+      return false;
+    }
     assertTrue(audioRecord != null);
     assertTrue(audioThread == null);
     try {
@@ -392,29 +409,101 @@ class WebRtcAudioRecord {
     audioThread = new AudioRecordThread("AudioRecordJavaThread");
     audioThread.start();
     scheduleLogRecordingConfigurationsTask(audioRecord);
+    recordingState = RecordingState.RECORDING;
     return true;
   }
 
   @CalledByNative
   private boolean stopRecording() {
     Logging.d(TAG, "stopRecording");
-    assertTrue(audioThread != null);
+    if (recordingState == RecordingState.IDLE || recordingState == RecordingState.STOPPED) {
+      return true;
+    }
+    stopAudioThread(/*releaseEffects=*/true, /*releaseResources=*/true);
+    recordingState = RecordingState.STOPPED;
+    return true;
+  }
+
+  @CalledByNative
+  boolean pauseRecording() {
+    Logging.d(TAG, "pauseRecording");
+    if (recordingState == RecordingState.PAUSED) {
+      Logging.d(TAG, "pauseRecording called while already paused");
+      return true;
+    }
+    if (recordingState != RecordingState.RECORDING) {
+      Logging.e(TAG, "pauseRecording invoked while not recording");
+      return false;
+    }
+    stopAudioThread(/*releaseEffects=*/false, /*releaseResources=*/false);
+    recordingState = RecordingState.PAUSED;
+    Logging.d(TAG, "pauseRecording succeeded");
+    return true;
+  }
+
+  @CalledByNative
+  boolean resumeRecording() {
+    Logging.d(TAG, "resumeRecording");
+    if (recordingState == RecordingState.RECORDING) {
+      Logging.d(TAG, "resumeRecording called while already recording");
+      return true;
+    }
+    if (recordingState != RecordingState.PAUSED) {
+      Logging.e(TAG, "resumeRecording invoked while not paused");
+      return false;
+    }
+    if (audioRecord == null) {
+      Logging.e(TAG, "resumeRecording failed: audioRecord is null");
+      recordingState = RecordingState.STOPPED;
+      return false;
+    }
+    assertTrue(audioThread == null);
+    try {
+      audioRecord.startRecording();
+    } catch (IllegalStateException e) {
+      reportWebRtcAudioRecordStartError(AudioRecordStartErrorCode.AUDIO_RECORD_START_EXCEPTION,
+          "AudioRecord.startRecording failed (resume): " + e.getMessage());
+      stopAudioThread(/*releaseEffects=*/true, /*releaseResources=*/true);
+      recordingState = RecordingState.STOPPED;
+      return false;
+    }
+    if (audioRecord.getRecordingState() != AudioRecord.RECORDSTATE_RECORDING) {
+      reportWebRtcAudioRecordStartError(AudioRecordStartErrorCode.AUDIO_RECORD_START_STATE_MISMATCH,
+          "AudioRecord.startRecording failed during resume - incorrect state: "
+              + audioRecord.getRecordingState());
+      stopAudioThread(/*releaseEffects=*/true, /*releaseResources=*/true);
+      recordingState = RecordingState.STOPPED;
+      return false;
+    }
+    audioThread = new AudioRecordThread("AudioRecordJavaThread");
+    audioThread.start();
+    scheduleLogRecordingConfigurationsTask(audioRecord);
+    recordingState = RecordingState.RECORDING;
+    Logging.d(TAG, "resumeRecording succeeded");
+    return true;
+  }
+
+  private void stopAudioThread(boolean releaseEffects, boolean releaseResources) {
     if (future != null) {
       if (!future.isDone()) {
-        // Might be needed if the client calls startRecording(), stopRecording() back-to-back.
         future.cancel(true /* mayInterruptIfRunning */);
       }
       future = null;
     }
-    audioThread.stopThread();
-    if (!ThreadUtils.joinUninterruptibly(audioThread, AUDIO_RECORD_THREAD_JOIN_TIMEOUT_MS)) {
-      Logging.e(TAG, "Join of AudioRecordJavaThread timed out");
-      WebRtcAudioUtils.logAudioState(TAG, context, audioManager);
+    if (audioThread != null) {
+      audioThread.stopThread();
+      if (!ThreadUtils.joinUninterruptibly(audioThread, AUDIO_RECORD_THREAD_JOIN_TIMEOUT_MS)) {
+        Logging.e(TAG, "Join of AudioRecordJavaThread timed out");
+        WebRtcAudioUtils.logAudioState(TAG, context, audioManager);
+      }
+      audioThread = null;
+    }
+    if (releaseEffects) {
+      effects.release();
+    }
+    if (releaseResources) {
+      releaseAudioResources();
     }
-    audioThread = null;
-    effects.release();
-    releaseAudioResources();
-    return true;
   }
 
   @TargetApi(Build.VERSION_CODES.M)
@@ -531,6 +620,11 @@ class WebRtcAudioRecord {
       audioRecord = null;
     }
     audioSourceMatchesRecordingSessionRef.set(null);
+    // Keep STOPPED semantics for all non-recording states to avoid treating a failed resume as
+    // PAUSED (which would assume resources are intact).
+    if (recordingState != RecordingState.RECORDING) {
+      recordingState = RecordingState.STOPPED;
+    }
   }
 
   private void reportWebRtcAudioRecordInitError(String errorMessage) {
diff --git a/sdk/android/src/jni/audio_device/aaudio_recorder.cc b/sdk/android/src/jni/audio_device/aaudio_recorder.cc
index 8867cf8a80..c3e0331372 100644
--- a/sdk/android/src/jni/audio_device/aaudio_recorder.cc
+++ b/sdk/android/src/jni/audio_device/aaudio_recorder.cc
@@ -100,6 +100,18 @@ int AAudioRecorder::StopRecording() {
   return 0;
 }
 
+int AAudioRecorder::PauseRecording() {
+  RTC_LOG(LS_INFO) << "PauseRecording";
+  RTC_LOG(LS_ERROR) << "PauseRecording not supported for AAudioRecorder";
+  return -1;
+}
+
+int AAudioRecorder::ResumeRecording() {
+  RTC_LOG(LS_INFO) << "ResumeRecording";
+  RTC_LOG(LS_ERROR) << "ResumeRecording not supported for AAudioRecorder";
+  return -1;
+}
+
 bool AAudioRecorder::Recording() const {
   return recording_;
 }
diff --git a/sdk/android/src/jni/audio_device/aaudio_recorder.h b/sdk/android/src/jni/audio_device/aaudio_recorder.h
index 5d6881ac3f..566a45c0cf 100644
--- a/sdk/android/src/jni/audio_device/aaudio_recorder.h
+++ b/sdk/android/src/jni/audio_device/aaudio_recorder.h
@@ -56,6 +56,8 @@ class AAudioRecorder : public AudioInput, public AAudioObserverInterface {
 
   int StartRecording() override;
   int StopRecording() override;
+  int PauseRecording() override;
+  int ResumeRecording() override;
   bool Recording() const override;
 
   void AttachAudioBuffer(AudioDeviceBuffer* audioBuffer) override;
diff --git a/sdk/android/src/jni/audio_device/audio_device_module.cc b/sdk/android/src/jni/audio_device/audio_device_module.cc
index 4331db2112..3bdb6454d5 100644
--- a/sdk/android/src/jni/audio_device/audio_device_module.cc
+++ b/sdk/android/src/jni/audio_device/audio_device_module.cc
@@ -306,6 +306,41 @@ class AndroidAudioDeviceModule : public AudioDeviceModule {
     return result;
   }
 
+  bool PauseRecording(JNIEnv* /*env*/) override {
+    RTC_DLOG(LS_INFO) << __FUNCTION__;
+    if (!initialized_)
+      return false;
+    if (!input_->SupportsPauseResume())
+      return false;
+    if (!Recording()) {
+      return true;
+    }
+    int32_t result = input_->PauseRecording();
+    RTC_DLOG(LS_INFO) << "input: " << result;
+    if (result == 0) {
+      audio_device_buffer_->StopRecording();
+    }
+    return result == 0;
+  }
+
+  bool ResumeRecording(JNIEnv* /*env*/) override {
+    RTC_DLOG(LS_INFO) << __FUNCTION__;
+    if (!initialized_)
+      return false;
+    if (!input_->SupportsPauseResume())
+      return false;
+    int32_t result = input_->ResumeRecording();
+    RTC_DLOG(LS_INFO) << "input: " << result;
+    if (result == 0) {
+      audio_device_buffer_->StartRecording();
+    }
+    return result == 0;
+  }
+
+  bool SupportsPauseRecording(JNIEnv* /*env*/) const override {
+    return input_->SupportsPauseResume();
+  }
+
   bool Recording() const override {
     RTC_DLOG(LS_INFO) << __FUNCTION__;
     return input_->Recording();
diff --git a/sdk/android/src/jni/audio_device/audio_device_module.h b/sdk/android/src/jni/audio_device/audio_device_module.h
index dc7c73d587..6309c8e7a4 100644
--- a/sdk/android/src/jni/audio_device/audio_device_module.h
+++ b/sdk/android/src/jni/audio_device/audio_device_module.h
@@ -37,7 +37,10 @@ class AudioInput {
 
   virtual int32_t StartRecording() = 0;
   virtual int32_t StopRecording() = 0;
+  virtual int32_t PauseRecording() = 0;
+  virtual int32_t ResumeRecording() = 0;
   virtual bool Recording() const = 0;
+  virtual bool SupportsPauseResume() const { return false; }
 
   virtual void AttachAudioBuffer(AudioDeviceBuffer* audioBuffer) = 0;
 
diff --git a/sdk/android/src/jni/audio_device/audio_record_jni.cc b/sdk/android/src/jni/audio_device/audio_record_jni.cc
index 43706e9a6d..8568c834e8 100644
--- a/sdk/android/src/jni/audio_device/audio_record_jni.cc
+++ b/sdk/android/src/jni/audio_device/audio_record_jni.cc
@@ -66,6 +66,7 @@ AudioRecordJni::AudioRecordJni(JNIEnv* env,
       frames_per_buffer_(0),
       initialized_(false),
       recording_(false),
+      paused_(false),
       audio_device_buffer_(nullptr) {
   RTC_LOG(LS_INFO) << "ctor";
   RTC_DCHECK(audio_parameters_.is_valid());
@@ -123,6 +124,7 @@ int32_t AudioRecordJni::InitRecording() {
                frames_per_buffer_ * bytes_per_frame);
   RTC_CHECK_EQ(frames_per_buffer_, audio_parameters_.frames_per_10ms_buffer());
   initialized_ = true;
+  paused_ = false;
   return 0;
 }
 
@@ -148,6 +150,7 @@ int32_t AudioRecordJni::StartRecording() {
     return -1;
   }
   recording_ = true;
+  paused_ = false;
   return 0;
 }
 
@@ -179,10 +182,44 @@ int32_t AudioRecordJni::StopRecording() {
   thread_checker_java_.Detach();
   initialized_ = false;
   recording_ = false;
+  paused_ = false;
   direct_buffer_address_ = nullptr;
   return 0;
 }
 
+int32_t AudioRecordJni::PauseRecording() {
+  RTC_LOG(LS_INFO) << "PauseRecording";
+  RTC_DCHECK(thread_checker_.IsCurrent());
+  if (!initialized_ || !recording_) {
+    return 0;
+  }
+  env_ = AttachCurrentThreadIfNeeded();
+  if (!Java_WebRtcAudioRecord_pauseRecording(env_, j_audio_record_)) {
+    RTC_LOG(LS_ERROR) << "PauseRecording failed";
+    return -1;
+  }
+  thread_checker_java_.Detach();
+  recording_ = false;
+  paused_ = true;
+  return 0;
+}
+
+int32_t AudioRecordJni::ResumeRecording() {
+  RTC_LOG(LS_INFO) << "ResumeRecording";
+  RTC_DCHECK(thread_checker_.IsCurrent());
+  if (!initialized_ || !paused_) {
+    return 0;
+  }
+  env_ = AttachCurrentThreadIfNeeded();
+  if (!Java_WebRtcAudioRecord_resumeRecording(env_, j_audio_record_)) {
+    RTC_LOG(LS_ERROR) << "ResumeRecording failed";
+    return -1;
+  }
+  recording_ = true;
+  paused_ = false;
+  return 0;
+}
+
 bool AudioRecordJni::Recording() const {
   return recording_;
 }
diff --git a/sdk/android/src/jni/audio_device/audio_record_jni.h b/sdk/android/src/jni/audio_device/audio_record_jni.h
index fa39318d36..865e3e1d0f 100644
--- a/sdk/android/src/jni/audio_device/audio_record_jni.h
+++ b/sdk/android/src/jni/audio_device/audio_record_jni.h
@@ -65,7 +65,10 @@ class AudioRecordJni : public AudioInput {
 
   int32_t StartRecording() override;
   int32_t StopRecording() override;
+  int32_t PauseRecording() override;
+  int32_t ResumeRecording() override;
   bool Recording() const override;
+  bool SupportsPauseResume() const override { return true; }
 
   void AttachAudioBuffer(AudioDeviceBuffer* audioBuffer) override;
 
@@ -130,6 +133,7 @@ class AudioRecordJni : public AudioInput {
   bool initialized_;
 
   bool recording_;
+  bool paused_;
 
   // Raw pointer handle provided to us in AttachAudioBuffer(). Owned by the
   // AudioDeviceModuleImpl class and called by AudioDeviceModule::Create().
diff --git a/sdk/android/src/jni/audio_device/java_audio_device_module.cc b/sdk/android/src/jni/audio_device/java_audio_device_module.cc
index a8152a39c4..db9db9afb7 100644
--- a/sdk/android/src/jni/audio_device/java_audio_device_module.cc
+++ b/sdk/android/src/jni/audio_device/java_audio_device_module.cc
@@ -58,5 +58,26 @@ static jlong JNI_JavaAudioDeviceModule_CreateAudioDeviceModule(
           .release());
 }
 
+[[maybe_unused]] static jboolean JNI_JavaAudioDeviceModule_nativePauseRecording(
+    JNIEnv* env,
+    jlong native_adm) {
+  auto* adm = reinterpret_cast<AudioDeviceModule*>(native_adm);
+  return adm && adm->PauseRecording(env);
+}
+
+[[maybe_unused]] static jboolean JNI_JavaAudioDeviceModule_nativeResumeRecording(
+    JNIEnv* env,
+    jlong native_adm) {
+  auto* adm = reinterpret_cast<AudioDeviceModule*>(native_adm);
+  return adm && adm->ResumeRecording(env);
+}
+
+[[maybe_unused]] static jboolean JNI_JavaAudioDeviceModule_nativeSupportsPauseRecording(
+    JNIEnv* env,
+    jlong native_adm) {
+  auto* adm = reinterpret_cast<AudioDeviceModule*>(native_adm);
+  return adm && adm->SupportsPauseRecording(env);
+}
+
 }  // namespace jni
 }  // namespace webrtc
diff --git a/sdk/android/src/jni/audio_device/opensles_recorder.cc b/sdk/android/src/jni/audio_device/opensles_recorder.cc
index b34cba9f23..3477583b5a 100644
--- a/sdk/android/src/jni/audio_device/opensles_recorder.cc
+++ b/sdk/android/src/jni/audio_device/opensles_recorder.cc
@@ -171,6 +171,18 @@ int OpenSLESRecorder::StopRecording() {
   return 0;
 }
 
+int OpenSLESRecorder::PauseRecording() {
+  ALOGD("PauseRecording[tid=%d]", webrtc::CurrentThreadId());
+  RTC_LOG(LS_ERROR) << "PauseRecording not supported for OpenSLESRecorder";
+  return -1;
+}
+
+int OpenSLESRecorder::ResumeRecording() {
+  ALOGD("ResumeRecording[tid=%d]", webrtc::CurrentThreadId());
+  RTC_LOG(LS_ERROR) << "ResumeRecording not supported for OpenSLESRecorder";
+  return -1;
+}
+
 bool OpenSLESRecorder::Recording() const {
   return recording_;
 }
diff --git a/sdk/android/src/jni/audio_device/opensles_recorder.h b/sdk/android/src/jni/audio_device/opensles_recorder.h
index 67e23a47f7..9161ce22ff 100644
--- a/sdk/android/src/jni/audio_device/opensles_recorder.h
+++ b/sdk/android/src/jni/audio_device/opensles_recorder.h
@@ -75,6 +75,8 @@ class OpenSLESRecorder : public AudioInput {
 
   int StartRecording() override;
   int StopRecording() override;
+  int PauseRecording() override;
+  int ResumeRecording() override;
   bool Recording() const override;
 
   void AttachAudioBuffer(AudioDeviceBuffer* audio_buffer) override;
