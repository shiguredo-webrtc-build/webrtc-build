diff --git a/sdk/objc/components/audio/RTCAudioSessionConfiguration.m b/sdk/objc/components/audio/RTCAudioSessionConfiguration.m
index 143a7864f8..9e013c142f 100644
--- a/sdk/objc/components/audio/RTCAudioSessionConfiguration.m
+++ b/sdk/objc/components/audio/RTCAudioSessionConfiguration.m
@@ -60,7 +60,7 @@ - (instancetype)init {
     // nonmixable, hence activating the session will interrupt any other
     // audio sessions which are also nonmixable.
     _category = AVAudioSessionCategoryAmbient;
-    _categoryOptions = AVAudioSessionCategoryOptionAllowBluetooth;
+    _categoryOptions = AVAudioSessionCategoryOptionDefaultToSpeaker;
 
     // Specify mode for two-way voice communication (e.g. VoIP).
     _mode = AVAudioSessionModeVoiceChat;
@@ -75,7 +75,7 @@ - (instancetype)init {
     // e.g. wired headset on iPhone 6.
     // TODO(henrika): add support for stereo if needed.
     _inputNumberOfChannels = kRTCAudioSessionPreferredNumberOfChannels;
-    _outputNumberOfChannels = kRTCAudioSessionPreferredNumberOfChannels;
+    _outputNumberOfChannels = 2;
   }
   return self;
 }
diff --git a/sdk/objc/native/src/audio/audio_device_ios.h b/sdk/objc/native/src/audio/audio_device_ios.h
index bbb4025694..a1d9dd77fd 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.h
+++ b/sdk/objc/native/src/audio/audio_device_ios.h
@@ -245,6 +245,8 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
   // and therefore outlives this object.
   AudioDeviceBuffer* audio_device_buffer_;
 
+  uint8_t play_channels_;
+
   // Contains audio parameters (sample rate, #channels, buffer size etc.) for
   // the playout and recording sides. These structure is set in two steps:
   // first, native sample rate and #channels are defined in Init(). Next, the
diff --git a/sdk/objc/native/src/audio/audio_device_ios.mm b/sdk/objc/native/src/audio/audio_device_ios.mm
index 0b34f66c8c..bd514c68d3 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.mm
+++ b/sdk/objc/native/src/audio/audio_device_ios.mm
@@ -104,6 +104,7 @@ static void LogDeviceInfo() {
       render_error_handler_(render_error_handler),
       disregard_next_render_error_(false),
       audio_device_buffer_(nullptr),
+      play_channels_(2),
       audio_unit_(nullptr),
       recording_(0),
       playing_(0),
@@ -462,15 +463,14 @@ static void LogDeviceInfo() {
                                           AudioBufferList* io_data) {
   RTC_DCHECK_RUN_ON(&io_thread_checker_);
   // Verify 16-bit, noninterleaved mono PCM signal format.
-  RTC_DCHECK_EQ(1, io_data->mNumberBuffers);
   AudioBuffer* audio_buffer = &io_data->mBuffers[0];
-  RTC_DCHECK_EQ(1, audio_buffer->mNumberChannels);
+  RTC_CHECK_EQ(audio_buffer->mNumberChannels, play_channels_);
 
   // Produce silence and give audio unit a hint about it if playout is not
   // activated.
   if (!playing_.load(std::memory_order_acquire)) {
     const size_t size_in_bytes = audio_buffer->mDataByteSize;
-    RTC_CHECK_EQ(size_in_bytes / VoiceProcessingAudioUnit::kBytesPerSample,
+    RTC_CHECK_EQ(size_in_bytes / VoiceProcessingAudioUnit::kBytesPerSample / play_channels_,
                  num_frames);
     *flags |= kAudioUnitRenderAction_OutputIsSilence;
     memset(static_cast<int8_t*>(audio_buffer->mData), 0, size_in_bytes);
@@ -514,7 +514,7 @@ static void LogDeviceInfo() {
   PlayoutDelay(playout_delay_ms);
 
   if (last_hw_output_latency_update_sample_count_ >=
-      playout_parameters_.sample_rate() * kHwLatencyUpdatePeriodSeconds) {
+      playout_parameters_.sample_rate() * kHwLatencyUpdatePeriodSeconds * play_channels_) {
     // We update the hardware output latency every kHwLatencyUpdatePeriodSeconds
     // seconds.
     hw_output_latency_.store(
@@ -525,16 +525,31 @@ static void LogDeviceInfo() {
   double output_latency_ = hw_output_latency_.load(std::memory_order_relaxed) +
       kMsToSecond * playout_parameters_.GetBufferSizeInMilliseconds();
 
+  int16_t* samples = static_cast<int16_t*>(audio_buffer->mData);
+  size_t num_samples = num_frames * play_channels_;
+
+  int16_t max_sample = 0;
+  int16_t min_sample = 0;
+
+  for (size_t i = 0; i < num_samples; ++i) {
+    int16_t s = samples[i];
+    if (s > max_sample) max_sample = s;
+    if (s < min_sample) min_sample = s;
+  }
+
+  if (max_sample >= 32760 || min_sample <= -32760) {
+    RTCLogWarning(@"⚠️ PCM is near clipping: max = %d, min = %d", max_sample, min_sample);
+  }
   // Read decoded 16-bit PCM samples from WebRTC (using a size that matches
   // the native I/O audio unit) and copy the result to the audio buffer in the
   // `io_data` destination.
   fine_audio_buffer_->GetPlayoutData(
       rtc::ArrayView<int16_t>(static_cast<int16_t*>(audio_buffer->mData),
-                              num_frames),
+                              num_frames * play_channels_),
       playout_delay_ms);
 
-  last_hw_output_latency_update_sample_count_ += num_frames;
-  total_playout_samples_count_.fetch_add(num_frames, std::memory_order_relaxed);
+  last_hw_output_latency_update_sample_count_ += num_frames * play_channels_;
+  total_playout_samples_count_.fetch_add(num_frames * play_channels_, std::memory_order_relaxed);
   total_playout_samples_duration_ms_.fetch_add(
       num_frames * 1000 / playout_parameters_.sample_rate(),
       std::memory_order_relaxed);
@@ -742,7 +757,6 @@ static void LogDeviceInfo() {
   RTC_DCHECK(audio_device_buffer_) << "AttachAudioBuffer must be called first";
   RTC_DCHECK_GT(playout_parameters_.sample_rate(), 0);
   RTC_DCHECK_GT(record_parameters_.sample_rate(), 0);
-  RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
   RTC_DCHECK_EQ(record_parameters_.channels(), 1);
   // Inform the audio device buffer (ADB) about the new audio format.
   audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
@@ -1184,17 +1198,25 @@ static void LogDeviceInfo() {
 }
 
 int32_t AudioDeviceIOS::StereoPlayoutIsAvailable(bool& available) {
-  available = false;
+  available = true;
   return 0;
 }
 
 int32_t AudioDeviceIOS::SetStereoPlayout(bool enable) {
-  RTC_LOG_F(LS_WARNING) << "Not implemented";
-  return -1;
+  if (enable)
+    play_channels_ = 2;
+  else
+    play_channels_ = 1;
+
+  return 0;
 }
 
 int32_t AudioDeviceIOS::StereoPlayout(bool& enabled) const {
-  enabled = false;
+  if (play_channels_ == 2)
+    enabled = true;
+  else
+    enabled = false;
+
   return 0;
 }
 
diff --git a/sdk/objc/native/src/audio/voice_processing_audio_unit.mm b/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
index 1bd895687d..b962d61627 100644
--- a/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
+++ b/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
@@ -58,22 +58,6 @@ static void LogStreamDescription(AudioStreamBasicDescription description) {
 // A VP I/O unit's bus 0 connects to output hardware (speaker).
 static const AudioUnitElement kOutputBus = 0;
 
-// Returns the automatic gain control (AGC) state on the processed microphone
-// signal. Should be on by default for Voice Processing audio units.
-static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
-  RTC_DCHECK(audio_unit);
-  UInt32 size = sizeof(*enabled);
-  OSStatus result =
-      AudioUnitGetProperty(audio_unit,
-                           kAUVoiceIOProperty_VoiceProcessingEnableAGC,
-                           kAudioUnitScope_Global,
-                           kInputBus,
-                           enabled,
-                           &size);
-  RTCLog(@"VPIO unit AGC: %u", static_cast<unsigned int>(*enabled));
-  return result;
-}
-
 VoiceProcessingAudioUnit::VoiceProcessingAudioUnit(
     bool bypass_voice_processing,
     bool detect_mute_speech,
@@ -99,7 +83,7 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   // I/O audio unit.
   AudioComponentDescription vpio_unit_description;
   vpio_unit_description.componentType = kAudioUnitType_Output;
-  vpio_unit_description.componentSubType = kAudioUnitSubType_VoiceProcessingIO;
+  vpio_unit_description.componentSubType = kAudioUnitSubType_RemoteIO;
   vpio_unit_description.componentManufacturer = kAudioUnitManufacturer_Apple;
   vpio_unit_description.componentFlags = 0;
   vpio_unit_description.componentFlagsMask = 0;
@@ -221,14 +205,88 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   RTC_DCHECK_GE(state_, kUninitialized);
   RTCLog(@"Initializing audio unit with sample rate: %f", sample_rate);
 
-  RTCAudioSession *session = [RTCAudioSession sharedInstance];
-  [session startVoiceProcessingAudioUnit: this];
-
   OSStatus result = noErr;
-  AudioStreamBasicDescription format = GetFormat(sample_rate);
-  UInt32 size = sizeof(format);
+
+
+  AudioStreamBasicDescription stream_format = {};
+  UInt32 size = sizeof(stream_format);
+
+  result = AudioUnitGetProperty(vpio_unit_,
+                                        kAudioUnitProperty_StreamFormat,
+                                        kAudioUnitScope_Output,
+                                        kOutputBus,
+                                        &stream_format,
+                                        &size);
+
+  if (result != noErr) {
+    RTCLogError(@"Failed to get stream format. Error = %ld.",
+                (long)result);
+    return false;
+  }
+  RTCLog(@"Output AudioStreamBasicDescription: {\n"
+          "  mSampleRate: %.2f\n"
+          "  mFormatID: '%c%c%c%c'\n"
+          "  mFormatFlags: 0x%08X\n"
+          "  mBytesPerPacket: %u\n"
+          "  mFramesPerPacket: %u\n"
+          "  mBytesPerFrame: %u\n"
+          "  mChannelsPerFrame: %u\n"
+          "  mBitsPerChannel: %u\n"
+          "  mReserved: %u\n}",
+        stream_format.mSampleRate,
+        (char)((stream_format.mFormatID >> 24) & 0xFF),
+        (char)((stream_format.mFormatID >> 16) & 0xFF),
+        (char)((stream_format.mFormatID >> 8) & 0xFF),
+        (char)(stream_format.mFormatID & 0xFF),
+        (unsigned int)stream_format.mFormatFlags,
+        (unsigned int)stream_format.mBytesPerPacket,
+        (unsigned int)stream_format.mFramesPerPacket,
+        (unsigned int)stream_format.mBytesPerFrame,
+        (unsigned int)stream_format.mChannelsPerFrame,
+        (unsigned int)stream_format.mBitsPerChannel,
+        (unsigned int)stream_format.mReserved);
+
+
+  result = AudioUnitGetProperty(vpio_unit_,
+                                        kAudioUnitProperty_StreamFormat,
+                                        kAudioUnitScope_Input,
+                                        kOutputBus,
+                                        &stream_format,
+                                        &size);
+
+  if (result != noErr) {
+    RTCLogError(@"Failed to get stream format. Error = %ld.",
+                (long)result);
+    return false;
+  }
+  RTCLog(@"Input AudioStreamBasicDescription: {\n"
+          "  mSampleRate: %.2f\n"
+          "  mFormatID: '%c%c%c%c'\n"
+          "  mFormatFlags: 0x%08X\n"
+          "  mBytesPerPacket: %u\n"
+          "  mFramesPerPacket: %u\n"
+          "  mBytesPerFrame: %u\n"
+          "  mChannelsPerFrame: %u\n"
+          "  mBitsPerChannel: %u\n"
+          "  mReserved: %u\n}",
+        stream_format.mSampleRate,
+        (char)((stream_format.mFormatID >> 24) & 0xFF),
+        (char)((stream_format.mFormatID >> 16) & 0xFF),
+        (char)((stream_format.mFormatID >> 8) & 0xFF),
+        (char)(stream_format.mFormatID & 0xFF),
+        (unsigned int)stream_format.mFormatFlags,
+        (unsigned int)stream_format.mBytesPerPacket,
+        (unsigned int)stream_format.mFramesPerPacket,
+        (unsigned int)stream_format.mBytesPerFrame,
+        (unsigned int)stream_format.mChannelsPerFrame,
+        (unsigned int)stream_format.mBitsPerChannel,
+        (unsigned int)stream_format.mReserved);
+
+  
+  AudioStreamBasicDescription output_format = GetFormat(sample_rate);
+  size = sizeof(output_format);
 #if !defined(NDEBUG)
-  LogStreamDescription(format);
+  LogStreamDescription(output_format);
 #endif
 
   // Set the format on the output scope of the input element/bus.
@@ -236,7 +294,7 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
                                 kAudioUnitProperty_StreamFormat,
                                 kAudioUnitScope_Output,
                                 kInputBus,
-                                &format,
+                                &output_format,
                                 size);
   if (result != noErr) {
     RTCLogError(@"Failed to set format on output scope of input bus. "
@@ -245,12 +303,16 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
     return false;
   }
 
+  AudioStreamBasicDescription input_format = GetFormat(sample_rate);
+  input_format.mChannelsPerFrame = 2;
+  input_format.mBytesPerPacket = kBytesPerSample * 2;
+  input_format.mBytesPerFrame = kBytesPerSample * 2;
   // Set the format on the input scope of the output element/bus.
   result = AudioUnitSetProperty(vpio_unit_,
                                 kAudioUnitProperty_StreamFormat,
                                 kAudioUnitScope_Input,
                                 kOutputBus,
-                                &format,
+                                &input_format,
                                 size);
   if (result != noErr) {
     RTCLogError(@"Failed to set format on input scope of output bus. "
@@ -284,106 +346,6 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   if (result == noErr) {
     RTCLog(@"Voice Processing I/O unit is now initialized.");
   }
-
-  if (detect_mute_speech_) {
-    if (@available(iOS 15, *)) {
-      // Set listener for muted speech event.
-      AUVoiceIOMutedSpeechActivityEventListener listener =
-          ^(AUVoiceIOSpeechActivityEvent event) {
-            observer_->OnReceivedMutedSpeechActivity(event);
-          };
-      result = AudioUnitSetProperty(
-          vpio_unit_,
-          kAUVoiceIOProperty_MutedSpeechActivityEventListener,
-          kAudioUnitScope_Global,
-          0,
-          &listener,
-          sizeof(AUVoiceIOMutedSpeechActivityEventListener));
-      if (result != noErr) {
-        RTCLog(
-            @"Failed to set muted speech activity event listener. Error=%ld.",
-            (long)result);
-      }
-    }
-  }
-
-  if (bypass_voice_processing_) {
-    // Attempt to disable builtin voice processing.
-    UInt32 toggle = 1;
-    result = AudioUnitSetProperty(vpio_unit_,
-                                  kAUVoiceIOProperty_BypassVoiceProcessing,
-                                  kAudioUnitScope_Global,
-                                  kInputBus,
-                                  &toggle,
-                                  sizeof(toggle));
-    if (result == noErr) {
-      RTCLog(@"Successfully bypassed voice processing.");
-    } else {
-      RTCLogError(@"Failed to bypass voice processing. Error=%ld.",
-                  (long)result);
-    }
-    state_ = kInitialized;
-    return true;
-  }
-
-  // AGC should be enabled by default for Voice Processing I/O units but it is
-  // checked below and enabled explicitly if needed. This scheme is used
-  // to be absolutely sure that the AGC is enabled since we have seen cases
-  // where only zeros are recorded and a disabled AGC could be one of the
-  // reasons why it happens.
-  int agc_was_enabled_by_default = 0;
-  UInt32 agc_is_enabled = 0;
-  result = GetAGCState(vpio_unit_, &agc_is_enabled);
-  if (result != noErr) {
-    RTCLogError(@"Failed to get AGC state (1st attempt). "
-                 "Error=%ld.",
-                (long)result);
-    // Example of error code: kAudioUnitErr_NoConnection (-10876).
-    // All error codes related to audio units are negative and are therefore
-    // converted into a postive value to match the UMA APIs.
-    RTC_HISTOGRAM_COUNTS_SPARSE_100000("WebRTC.Audio.GetAGCStateErrorCode1",
-                                       (-1) * result);
-  } else if (agc_is_enabled) {
-    // Remember that the AGC was enabled by default. Will be used in UMA.
-    agc_was_enabled_by_default = 1;
-  } else {
-    // AGC was initially disabled => try to enable it explicitly.
-    UInt32 enable_agc = 1;
-    result = AudioUnitSetProperty(vpio_unit_,
-                                  kAUVoiceIOProperty_VoiceProcessingEnableAGC,
-                                  kAudioUnitScope_Global,
-                                  kInputBus,
-                                  &enable_agc,
-                                  sizeof(enable_agc));
-    if (result != noErr) {
-      RTCLogError(@"Failed to enable the built-in AGC. "
-                   "Error=%ld.",
-                  (long)result);
-      RTC_HISTOGRAM_COUNTS_SPARSE_100000("WebRTC.Audio.SetAGCStateErrorCode",
-                                         (-1) * result);
-    }
-    result = GetAGCState(vpio_unit_, &agc_is_enabled);
-    if (result != noErr) {
-      RTCLogError(@"Failed to get AGC state (2nd attempt). "
-                   "Error=%ld.",
-                  (long)result);
-      RTC_HISTOGRAM_COUNTS_SPARSE_100000("WebRTC.Audio.GetAGCStateErrorCode2",
-                                         (-1) * result);
-    }
-  }
-
-  // Track if the built-in AGC was enabled by default (as it should) or not.
-  RTC_HISTOGRAM_BOOLEAN("WebRTC.Audio.BuiltInAGCWasEnabledByDefault",
-                        agc_was_enabled_by_default);
-  RTCLog(@"WebRTC.Audio.BuiltInAGCWasEnabledByDefault: %d",
-         agc_was_enabled_by_default);
-  // As a final step, add an UMA histogram for tracking the AGC state.
-  // At this stage, the AGC should be enabled, and if it is not, more work is
-  // needed to find out the root cause.
-  RTC_HISTOGRAM_BOOLEAN("WebRTC.Audio.BuiltInAGCIsEnabled", agc_is_enabled);
-  RTCLog(@"WebRTC.Audio.BuiltInAGCIsEnabled: %u",
-         static_cast<unsigned int>(agc_is_enabled));
-
   state_ = kInitialized;
   return true;
 }
@@ -407,9 +369,6 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   RTC_DCHECK_GE(state_, kUninitialized);
   RTCLog(@"Stopping audio unit.");
 
-  RTCAudioSession *session = [RTCAudioSession sharedInstance];
-  [session stopVoiceProcessingAudioUnit];
-
   OSStatus result = AudioOutputUnitStop(vpio_unit_);
   if (result != noErr) {
     RTCLogError(@"Failed to stop audio unit. Error=%ld", (long)result);
